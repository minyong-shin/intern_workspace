size[i]<-length(which(distdata[,col+1]==i))
}
print(size)
print(w)
print(distdata[,col+1])
distdata;
}
disw<-function(data,w,row,k){
gnum<-0
dis<-0
for(i in 1:row){
for(j in 1:k){
dis[j]<-dist(rbind(data[i,],w[j,]))
}
gnum[i]<-which.min(dis)
}
distdata<-cbind(data,gnum)
distdata;
}
divG<-function(data,w,k,row,col){
group<-col+1
for(i in 1:k){
for(j in 1:col){
if(length(which(data[,group]==i))==0){
w[i,j] <- w[i,j]
}
else if(length(which(data[,group]==i))==1){
w[i,j]<-data[which(data[,group]==i),-(group)][j]
}else{
w[i,j]<-mean(data[which(data[,group]==i),-(group)][,j])
}
}
}
w;
}
k_means(data,w,k)
distdata<-disW(data,w,row,k)
disW(data,w,row,k)
divG(distdata,w,k,row,col)
divG
?disW
?disW
?disW
row<-nrow(data)
data<-as.matrix(iris[,-5])
row<-nrow(data)
col<-ncol(data)
k<-3#군집수 결정
sample<-sample(row,k)
w<-data[sample,]
k_means<-function(data,w,k){
row<-nrow(data)
col<-ncol(data)
sample2<-w
w1<-0
while(identical(w,w1)==FALSE){
w<-sample2
distdata<-dist(data,w,row,k)
w1<-divG(distdata,w,k,row,col)
sample2<-w1
}
size<-0
for(i in 1:k){
size[i]<-length(which(distdata[,col+1]==i))
}
print(size)
print(w)
print(distdata[,col+1])
distdata;
}
disw<-function(data,w,row,k){
gnum<-0
dis<-0
for(i in 1:row){
for(j in 1:k){
dis[j]<-dist(rbind(data[i,],w[j,]))
}
gnum[i]<-which.min(dis)
}
distdata<-cbind(data,gnum)
distdata;
}
divG<-function(data,w,k,row,col){
group<-col+1
for(i in 1:k){
for(j in 1:col){
if(length(which(data[,group]==i))==0){
w[i,j] <- w[i,j]
}
else if(length(which(data[,group]==i))==1){
w[i,j]<-data[which(data[,group]==i),-(group)][j]
}else{
w[i,j]<-mean(data[which(data[,group]==i),-(group)][,j])
}
}
}
w;
}
k_means(data,w,k)
k_means<-function(data,w,k){
row<-nrow(data)
col<-ncol(data)
sample2<-w
w1<-0
while(identical(w,w1)==FALSE){
w<-sample2
distdata<-disw(data,w,row,k)
w1<-divG(distdata,w,k,row,col)
sample2<-w1
}
size<-0
for(i in 1:k){
size[i]<-length(which(distdata[,col+1]==i))
}
print(size)
print(w)
print(distdata[,col+1])
distdata;
}
disw<-function(data,w,row,k){
gnum<-0
dis<-0
for(i in 1:row){
for(j in 1:k){
dis[j]<-dist(rbind(data[i,],w[j,]))
}
gnum[i]<-which.min(dis)
}
distdata<-cbind(data,gnum)
distdata;
}
divG<-function(data,w,k,row,col){
group<-col+1
for(i in 1:k){
for(j in 1:col){
if(length(which(data[,group]==i))==0){
w[i,j] <- w[i,j]
}
else if(length(which(data[,group]==i))==1){
w[i,j]<-data[which(data[,group]==i),-(group)][j]
}else{
w[i,j]<-mean(data[which(data[,group]==i),-(group)][,j])
}
}
}
w;
}
k_means(data,w,k)
kmeans(data,k)
k_means(data,W,k)
k_means(data,w,k)
library(stringr)
install.packages("stringr")
y
library(stringr)
url="http://www.hollys.co.kr/store/korea/korStore.do"
cont = readLines(url,encoding = "UTF-8")
index = which(str_detect(cont, "<tr class=\"\">")==TRUE)
clean = function(words){
words = gsub("\t*<.*?>\t*","",words)
return(words)
}
library(dplyr)
cont %>% head(10)
where = cont[index+1]
where = clean(where)
name = cont[index+2]
name
name = clean(name)
name
now = cont[index+3]
now = clean(now)
add = cont[index+4]
add = clean(add)
phone = cont[index+8]
phone = clean(phone)
where_list = str_extract_all(where, "[가-힣]+")
where_list[[1]][2]
df = data.frame(where, name, now, add, phone)
df
install.packages("rvest")
library(rvest)
url="http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=102"
cont = read_html(url)
xx=NULL
?rbinom
rbinom(1,1000,0.5)
rbinom(1,1000,0.5)
rbinom(1,1000,0.5)
rbinom(1,1000,0.5)
rbinom(2,1000,0.5)
rbinom(3,1000,0.5)
rbinom(1,500,0.5)
rbinom(1,1000,0.5)
plot(rbinom(1,1000,0.5))
plot(rbinom(100,1000,0.5))
plot(rbinom(100000,1000,0.5))
for(n in 1:100000){
mm=rbinom(1,1000,0.5)
xx[n]=(mm-500)/sqrt(250)
}
hist(xx)
xx=NULL
for(n in 1:100000){
mm=rbinom(1,1000,0.5)
xx[n]=(mm-500)/sqrt(250)
}
hist(xx)
xx[1]
xx[2]
xx[3]
hist(xx,main = "분포 히스토그램")
load("C:/Users/tlsal/Downloads/데마/myEnvironment.RData")
library(caret)
library(xgboost)
library(readr)
library(dplyr)
library(tidyr)
xgb_pred2<-predict(xgb.model_4_col,apt.te11[,-45])
library(caret)
install.packages("caret")
library(caret)
library(xgboost)
library(readr)
xgb_grid_3 <- expand.grid(
nrounds = 500,
eta = 0.23,
gamma = 0.24,
max_depth = 10,
min_child_weight = 7,
colsample_bytree = 1,
subsample = 1
)
control_3 = trainControl(method='cv', search='grid', number=5)
library(caret)
install.packages("wordVectors")
install.packages("devtools")
library(devtools)
install.packages("wordVectors")
library(devtools)
install.packages("wordVectors")
#GBM
library(rsample)      # data splitting
#GBM
install.packages("AwesHousing")
#GBM
install.packages("AmesHousing")
library(AmesHousing)
ah <- AmesHousing
ah <- AmesHousing::make_ames()
str(ah)
library(gbm)          # basic implementation
install.packages(c("gbm","h2o","pdp",'lime'))
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
str(ah)
set.seed(123)
idx = createDataPartition(ah,p = 0.7)
idx = createDataPartition(ah$Sale_Price,p = 0.7,list = F)
ah_train = ah[idx,]
ah_test =ah[-idx,]
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train,
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
gbm.fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ah_train,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train,
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001,
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
gbm.fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ah_train,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = TRUE
)
print(gbm.fit)
ah.gbm = predict(gbm.fit,ah_test)
R2(ah.gbm,ah_test$Sale_Price)
setwd("C:/Users/tlsal/Desktop/히어로네이션/R - 신체치수 모델링")
#신체치수데이터를 이용한 gbm
data <- read.csv("data2.csv")
data$sex <- ifelse(data$sex=="남", "M","F")
data$sex<-as.factor(data$sex)
#치수별로 모델분할
neck <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","neck")]
arm <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","arm")]
armhole <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","armhole")]
top_length <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","top_length")]
wrist <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","wrist")]
arm_round <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","arm_round")]
bottom_leng <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","bottom_leng")]
ankle <-data[,c("sex","age","height","weight","shoulder","chest","belly","waist","hip","thigh","ankle")]
#데이터 분할
idx2 = createDataPartition(neck$neck,p=0.7,list = F)
#데이터 분할
idx_neck <-createDataPartition(neck$neck, p=.8, list = F)
train_neck <-neck[idx_neck,]
test_neck <-neck[-idx_neck,]
#neck_gbm
neck_gbm_model <- gbm(
formula = neck ~ .,
distribution = "gaussian",
data = train_neck,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = TRUE
)
neck_pred<-predict(neck_gbm_model,test_neck)
R2(neck_pred,test_neck$neck)
control = trainControl(method='repeatedcv', search='random', number=3, verbose = TRUE, repeats = 3)
xgb.model_neck <- train(
neck~sex+age+height+weight+shoulder+chest+belly+waist+hip+thigh,
data=train_neck,trControl = control,method = 'xgbTree')
neck.xgb = predict(xgb.model_neck,test_neck)
R2(neck.xgb,test_neck$neck)#0.83
R2(neck_pred,test_neck$neck)
mean(neck_pred, neck.xgb)
neck_pred
neck.xgb
(neck_pred+neck.xgb)/2
mean_neck <- (neck_pred+neck.xgb)/2
R2(mean_neck,test_neck$neck)
R2(neck_pred,test_neck$neck)
R2(neck.xgb,test_neck$neck)#0.83
sqrt(min(gbm.fit$cv.error))
#손실함수 그래프 ntrees added to the ensemble
gbm.perf(gbm.fit,method = "cv")
sum(is.na(ah_train))
#neck_gbm
neck_gbm_model <- gbm(
formula = neck ~ .,
distribution = "gaussian",
data = train_neck,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = TRUE
)
neck_pred<-predict(neck_gbm_model,test_neck)
R2(neck_pred,test_neck$neck)
mean_neck <- (neck_pred+neck.xgb)/2
R2(mean_neck,test_neck$neck) #앙상블을 통해서 증가되는 결정계수는 약 0.5%임
sqrt(min(neck_gbm_model$cv.error))
#parameter tuning
gbm.fit <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ah_train,
n.trees = 5000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = TRUE
)
min_mse <- which.min(gbm.fit$cv.error)
min_mse
sqrt(min(gbm.fit$cv.error[min_mse]))
#ntree의 어느 부분에서 가장 최적일지
gbm.perf(gbm.fit,method="cv")
#rmse가 줄어든 것을 확인할 수 있다.
#gbm random search가 있을 것 같은데 일단 현재 문서에서는 grid search를 하므로
#grid search를 통해서 최적의 hyper parameter을 찾는다
# modify hyperparameter grid
hyper_grid <- expand.grid(
shrinkage = c(.01, .05, .1),
interaction.depth = c(3, 5, 7),
n.minobsinnode = c(5, 7, 10),
bag.fraction = c(.65, .8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
nrow(hyper_grid)
# total number of combinations
nrow(hyper_grid)
#신체치수데이터를 이용한 gbm
data <- read.csv("data2.csv")
#caret 패키지의 train함수처럼 자동으로 grid.search를 진행해주는 형식이 아니라
#직접 반복문을 통해서 입력해야한다.
# grid search
for(i in 1:nrow(hyper_grid)) {
# reproducibility
set.seed(123)
# train model
gbm.tune <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = random_ames_train,
n.trees = 6000,
interaction.depth = hyper_grid$interaction.depth[i],
shrinkage = hyper_grid$shrinkage[i],
n.minobsinnode = hyper_grid$n.minobsinnode[i],
bag.fraction = hyper_grid$bag.fraction[i],
train.fraction = .75,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
# add min training error and trees to grid
hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}
#caret 패키지의 train함수처럼 자동으로 grid.search를 진행해주는 형식이 아니라
#직접 반복문을 통해서 입력해야한다.
# grid search
#random sampling를 하는데 여기선 똑같은 train데이터를 추출하는데
#random으로 섞어서 추추
random_index <- sample(1:nrow(ah_train), nrow(ah_train))
random_ames_train <- ah_train[random_index, ]
#따라서 해당 결과의 하이퍼 파라미터를 확인하면 아래와 같ㄷ
# train GBM model
gbm.fit.final <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ames_train,
n.trees = 483,
interaction.depth = 5,
shrinkage = 0.1,
n.minobsinnode = 5,
bag.fraction = .65,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
#따라서 해당 결과의 하이퍼 파라미터를 확인하면 아래와 같다ㅏ
# train GBM model
gbm.fit.final <- gbm(
formula = Sale_Price ~ .,
distribution = "gaussian",
data = ah_train,
n.trees = 483,
interaction.depth = 5,
shrinkage = 0.1,
n.minobsinnode = 5,
bag.fraction = .65,
train.fraction = 1,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
#변수별로 종속변수에 관련이 높은 변수를 추출해준다
#즉,rmse에 영향을 미치는 변수를 추출 method는 두개로 사용형태가 다름
par(mar = c(5, 8, 1, 1))
summary(
gbm.fit.final,
cBars = 10,
method = relative.influence, # also can use permutation.test.gbm
las = 2
)
summary(
gbm.fit.final,
cBars = 5,
method = relative.influence, # also can use permutation.test.gbm
las = 2
)
#신체치수데이터를 이용한 gbm
data <- read.csv("data2.csv")
summary(
gbm.fit.final,
cBars = 10,
method = relative.influence, # also can use permutation.test.gbm
las = 2
)
#lime 라임 알고리즘
model_type.gbm <- function(x, ...) {
return("regression")
}
predict_model.gbm <- function(x, newdata, ...) {
pred <- predict(x, newdata, n.trees = x$n.trees)
return(as.data.frame(pred))
}
str(ah_test)
explainer <- lime(ah_train, gbm.fit.final)
library(lime)
explainer <- lime(ah_train, gbm.fit.final)
explanation <- explain(local_obs, explainer, n_features = 5)
local_obs <- ah_test[1:2, ]
explanation <- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)
local_obs <- ah_test[1:3, ]
explainer <- lime(ah_train, gbm.fit.final)
explanation <- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)
